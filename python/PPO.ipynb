{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps =  1.1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"ballcatcher3/Run4\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ballcatcher3\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 100 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 4 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.15 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 20480 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 1e-5 # Model learning rate.\n",
    "hidden_units = 96 # Number of units in hidden layer.\n",
    "batch_size = 128 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "# num_layers = 1\n",
    "# hidden_units = 10\n",
    "# time_horizon = 1000\n",
    "# learning_rate = 1e-3, 1e-4, 1e-5\n",
    "# normalize = True\n",
    "\n",
    "# could try:\n",
    "# num_epoch = 2\n",
    "# beta = 0\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'CatcherAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: CatcherAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 16\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 24\n",
      "        Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file,worker_id=4,base_port=5009)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Mean Reward: -25.871764269359982. Std of Reward: 6.895576667644034.\n",
      "Step: 20000. Mean Reward: -26.215395162254293. Std of Reward: 13.433948457543913.\n",
      "Step: 30000. Mean Reward: -20.656354106166685. Std of Reward: 21.639968079226684.\n",
      "Step: 40000. Mean Reward: -23.753837124465285. Std of Reward: 20.251614216236035.\n",
      "Step: 50000. Mean Reward: -16.621398162237465. Std of Reward: 25.316992150201603.\n",
      "Saved Model\n",
      "Step: 60000. Mean Reward: -13.95938583705287. Std of Reward: 26.138815425497253.\n",
      "Step: 70000. Mean Reward: -8.033079626260903. Std of Reward: 26.671862850913566.\n",
      "Step: 80000. Mean Reward: -6.25707615498313. Std of Reward: 27.52745696505806.\n",
      "Step: 90000. Mean Reward: -1.0055950770160298. Std of Reward: 24.732343974523822.\n",
      "Step: 100000. Mean Reward: 1.0991365573814968. Std of Reward: 24.720188217866927.\n",
      "Saved Model\n",
      "Step: 110000. Mean Reward: -4.126249864592809. Std of Reward: 26.60331805633026.\n",
      "Step: 120000. Mean Reward: 1.0567074616584502. Std of Reward: 24.963451824966636.\n",
      "Step: 130000. Mean Reward: 2.8473486142423368. Std of Reward: 25.094408078393247.\n",
      "Step: 140000. Mean Reward: 3.393699319609682. Std of Reward: 24.886331390485836.\n",
      "Step: 150000. Mean Reward: -2.755689511586317. Std of Reward: 26.27119250523135.\n",
      "Saved Model\n",
      "Step: 160000. Mean Reward: 4.437538603015268. Std of Reward: 25.563993025265116.\n",
      "Step: 170000. Mean Reward: 0.07974804894105512. Std of Reward: 26.470490213335932.\n",
      "Step: 180000. Mean Reward: 1.1498829083749964. Std of Reward: 25.075370900659884.\n",
      "Step: 190000. Mean Reward: 2.5119422548759514. Std of Reward: 24.51835293957791.\n",
      "Step: 200000. Mean Reward: 3.4039917224201726. Std of Reward: 24.965203168393682.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: 5.683254109714275. Std of Reward: 23.973878647018967.\n",
      "Step: 220000. Mean Reward: 4.383008640406614. Std of Reward: 22.49613582573538.\n",
      "Step: 230000. Mean Reward: 7.338098731943644. Std of Reward: 23.870986099010317.\n",
      "Step: 240000. Mean Reward: 8.837397432054782. Std of Reward: 22.944037985808038.\n",
      "Step: 250000. Mean Reward: 7.248120462977439. Std of Reward: 24.48626511208949.\n",
      "Saved Model\n",
      "Step: 260000. Mean Reward: 3.2488493343492113. Std of Reward: 25.16765075537384.\n",
      "Step: 270000. Mean Reward: 5.518884735076913. Std of Reward: 24.238938270356467.\n",
      "Step: 280000. Mean Reward: 4.1018652161269555. Std of Reward: 26.08961765608851.\n",
      "Step: 290000. Mean Reward: 1.3627133030697136. Std of Reward: 28.138864256735122.\n",
      "Step: 300000. Mean Reward: 5.0338493443809735. Std of Reward: 24.93202720791439.\n",
      "Saved Model\n",
      "Step: 310000. Mean Reward: 5.985413944993104. Std of Reward: 25.5129036212846.\n",
      "Step: 320000. Mean Reward: 6.249016565770349. Std of Reward: 23.403444896616744.\n",
      "Step: 330000. Mean Reward: 15.044352051111122. Std of Reward: 18.606050402284783.\n",
      "Step: 340000. Mean Reward: 11.359778642329125. Std of Reward: 20.989473123745814.\n",
      "Step: 350000. Mean Reward: 11.34123347861331. Std of Reward: 19.468833375025987.\n",
      "Saved Model\n",
      "Step: 360000. Mean Reward: 14.326437295185663. Std of Reward: 18.73282258993247.\n",
      "Step: 370000. Mean Reward: 18.49473136365592. Std of Reward: 14.070555643167653.\n",
      "Step: 380000. Mean Reward: 13.97143307360977. Std of Reward: 20.170770703121782.\n",
      "Step: 390000. Mean Reward: 14.76880828106976. Std of Reward: 18.545944984051555.\n",
      "Step: 400000. Mean Reward: 16.798889050828265. Std of Reward: 17.249182297465378.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: 14.9900001537297. Std of Reward: 19.838593711802506.\n",
      "Step: 420000. Mean Reward: 17.46513604586959. Std of Reward: 16.49814631087842.\n",
      "Step: 430000. Mean Reward: 15.497944611288935. Std of Reward: 17.834626515791772.\n",
      "Step: 440000. Mean Reward: 18.704506868233217. Std of Reward: 15.41567267542443.\n",
      "Step: 450000. Mean Reward: 20.254267387620732. Std of Reward: 11.951543754714601.\n",
      "Saved Model\n",
      "Step: 460000. Mean Reward: 19.623272075834134. Std of Reward: 15.42372949834302.\n",
      "Step: 470000. Mean Reward: 19.029613869081558. Std of Reward: 14.815303153002269.\n",
      "Step: 480000. Mean Reward: 19.28276101269566. Std of Reward: 15.879990869716016.\n",
      "Step: 490000. Mean Reward: 18.88038476783804. Std of Reward: 15.993955095593233.\n",
      "Step: 500000. Mean Reward: 21.849562416080843. Std of Reward: 12.29628709287097.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: 16.984789177181423. Std of Reward: 19.84380884891697.\n",
      "Step: 520000. Mean Reward: 18.875500169133336. Std of Reward: 17.298615168212777.\n",
      "Step: 530000. Mean Reward: 20.981559463647464. Std of Reward: 14.090833662735315.\n",
      "Step: 540000. Mean Reward: 20.931389964447483. Std of Reward: 13.84587607647704.\n",
      "Step: 550000. Mean Reward: 20.162143009113567. Std of Reward: 14.690301262384617.\n",
      "Saved Model\n",
      "Step: 560000. Mean Reward: 19.84984391636113. Std of Reward: 15.988473545384968.\n",
      "Step: 570000. Mean Reward: 21.00615399841027. Std of Reward: 14.207276269711498.\n",
      "Step: 580000. Mean Reward: 21.55661869764105. Std of Reward: 13.224618018125993.\n",
      "Step: 590000. Mean Reward: 18.146406372750004. Std of Reward: 18.081511525345157.\n",
      "Step: 600000. Mean Reward: 19.905775414987342. Std of Reward: 15.636252311461714.\n",
      "Saved Model\n",
      "Step: 610000. Mean Reward: 18.175883502812727. Std of Reward: 19.39273418175862.\n",
      "Step: 620000. Mean Reward: 22.07953402691525. Std of Reward: 12.971341380978302.\n",
      "Step: 630000. Mean Reward: 22.63216227753516. Std of Reward: 11.746601147044966.\n",
      "Step: 640000. Mean Reward: 23.053634062663157. Std of Reward: 10.590211074151433.\n",
      "Step: 650000. Mean Reward: 20.922113689009475. Std of Reward: 15.002682903314229.\n",
      "Saved Model\n",
      "Step: 660000. Mean Reward: 19.772727412288386. Std of Reward: 17.433163326450934.\n",
      "Step: 670000. Mean Reward: 21.17058396081439. Std of Reward: 14.596892400918772.\n",
      "Step: 680000. Mean Reward: 22.13709690024814. Std of Reward: 14.16769608407779.\n",
      "Step: 690000. Mean Reward: 23.784680735594716. Std of Reward: 10.026591473638357.\n",
      "Step: 700000. Mean Reward: 24.22860937359825. Std of Reward: 7.947910903182956.\n",
      "Saved Model\n",
      "Step: 710000. Mean Reward: 23.64050356972082. Std of Reward: 8.865181674171149.\n",
      "Step: 720000. Mean Reward: 24.89441469208447. Std of Reward: 4.996703442301819.\n",
      "Step: 730000. Mean Reward: 22.842953599553493. Std of Reward: 11.61457072346372.\n",
      "Step: 740000. Mean Reward: 23.410285811934703. Std of Reward: 10.85939102812214.\n",
      "Step: 750000. Mean Reward: 22.68300214144262. Std of Reward: 12.756609437955657.\n",
      "Saved Model\n",
      "Step: 760000. Mean Reward: 23.112651766434453. Std of Reward: 11.479985228874733.\n",
      "Step: 770000. Mean Reward: 24.088635569606254. Std of Reward: 8.56424941153653.\n",
      "Step: 780000. Mean Reward: 23.604385018460327. Std of Reward: 10.179978453042011.\n",
      "Step: 790000. Mean Reward: 23.49042397875915. Std of Reward: 9.988603881332084.\n",
      "Step: 800000. Mean Reward: 23.276804452978087. Std of Reward: 9.99791029909201.\n",
      "Saved Model\n",
      "Step: 810000. Mean Reward: 25.267150159076497. Std of Reward: 4.992499631982224.\n",
      "Step: 820000. Mean Reward: 25.382190289095114. Std of Reward: 4.618202085882528.\n",
      "Step: 830000. Mean Reward: 24.479481334305486. Std of Reward: 6.85865828384644.\n",
      "Step: 840000. Mean Reward: 24.745470991612194. Std of Reward: 6.828756094365629.\n",
      "Step: 850000. Mean Reward: 24.590134292263095. Std of Reward: 6.218059963637041.\n",
      "Saved Model\n",
      "Step: 860000. Mean Reward: 25.006181070007713. Std of Reward: 5.0405905425214526.\n",
      "Step: 870000. Mean Reward: 24.947661568082694. Std of Reward: 5.47661954225982.\n",
      "Step: 880000. Mean Reward: 24.85573610522843. Std of Reward: 5.391386397214461.\n",
      "Step: 890000. Mean Reward: 23.698897227421757. Std of Reward: 9.843091987706526.\n",
      "Step: 900000. Mean Reward: 23.263554925180838. Std of Reward: 10.368992060354982.\n",
      "Saved Model\n",
      "Step: 910000. Mean Reward: 22.060453835872263. Std of Reward: 13.470460572002848.\n",
      "Step: 920000. Mean Reward: 23.404320672586262. Std of Reward: 10.243316027022065.\n",
      "Step: 930000. Mean Reward: 23.819935122389612. Std of Reward: 8.560449360777458.\n",
      "Step: 940000. Mean Reward: 23.38406663270622. Std of Reward: 10.773590350821172.\n",
      "Step: 950000. Mean Reward: 22.752189073150237. Std of Reward: 12.477353392911493.\n",
      "Saved Model\n",
      "Step: 960000. Mean Reward: 23.621849471017544. Std of Reward: 8.98799179061852.\n",
      "Step: 970000. Mean Reward: 23.547392270386652. Std of Reward: 10.056353633900386.\n",
      "Step: 980000. Mean Reward: 24.25524907284928. Std of Reward: 7.313924484675984.\n",
      "Step: 990000. Mean Reward: 24.656173815604152. Std of Reward: 5.536265457491006.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000000. Mean Reward: 24.807688622686133. Std of Reward: 5.726366692107065.\n",
      "Saved Model\n",
      "Step: 1010000. Mean Reward: 23.846339457446035. Std of Reward: 9.052075206076283.\n",
      "Step: 1020000. Mean Reward: 24.633311524485247. Std of Reward: 5.5792956295574445.\n",
      "Step: 1030000. Mean Reward: 24.466849128350237. Std of Reward: 6.6522778248465055.\n",
      "Step: 1040000. Mean Reward: 24.919018547757453. Std of Reward: 4.0874435618459595.\n",
      "Step: 1050000. Mean Reward: 24.74792303777007. Std of Reward: 4.480822913176565.\n",
      "Saved Model\n",
      "Step: 1060000. Mean Reward: 24.11559599311562. Std of Reward: 7.710909254237402.\n",
      "Step: 1070000. Mean Reward: 24.95741972715663. Std of Reward: 4.276037706533414.\n",
      "Step: 1080000. Mean Reward: 24.859974736647782. Std of Reward: 3.4228790050727684.\n",
      "Step: 1090000. Mean Reward: 24.713693779405098. Std of Reward: 4.668712893459659.\n",
      "Step: 1100000. Mean Reward: 24.92757838646914. Std of Reward: 2.5270201254859526.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 24.77508436453969. Std of Reward: 3.7210972587016373.\n",
      "Step: 1120000. Mean Reward: 23.78485354782165. Std of Reward: 8.20411779078136.\n",
      "Step: 1130000. Mean Reward: 23.960778068942794. Std of Reward: 7.394238808646003.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-effd4f9335ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# Perform gradient descent with experience buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# Write training statistics to tensorboard.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\PPO-Reinforcement-Learning\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mupdate_model\u001b[1;34m(self, batch_size, num_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_in\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'observations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 v_loss, p_loss, _ = self.sess.run([self.model.value_loss, self.model.policy_loss,\n\u001b[1;32m--> 181\u001b[1;33m                                                    self.model.update_batch], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m    182\u001b[0m                 \u001b[0mtotal_v\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mtotal_p\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/ppo/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/ballcatcher3/Run4\\model-1100000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/ballcatcher3/Run4\\model-1100000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
